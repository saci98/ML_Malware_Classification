import os.path
import sys
from datetime import datetime
import zipfile
from typing import Any

import numpy as np
from scipy.sparse import save_npz, csr_matrix
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
from sklearn import metrics, svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve
from matplotlib import pyplot as plt
import seaborn as sns
from Utils.Extract_feature import extract_ngrams, extract_bytes_sequence, extract_opcode_sequence, count_asm_opcode, \
    extract_syscall, save_syscall, check_syscall, load_syscall, extract_syscall_sequence, extract_entropy, extract_apis, \
    save_apis, load_apis, check_apis, extract_binary_sizes, extract_entropy_section, extract_bytes_microsoft
from pathlib import Path
import pandas as pd
import argparse

from Utils.IndividualScoreStats import IndividualScoreStats
from Utils.Integrate_features import integrated_features

data_dir = Path("data")
smp_dir = data_dir.joinpath("samples")
ftr_dir = Path("pre-extract")
if not ftr_dir.exists():
    ftr_dir.mkdir()
if not Path("results").exists():
        Path("results").mkdir()


# def extract_files():
#     dataset = {
#         "Id": [],
#         "Class": []
#     }
#     families = ["IcedId", "Magniber", "Ramnit", "Stop", "Trickbot", "Virlock", "Wannacry", "Zbot", "Benign"]
#     for i, family in enumerate(families):
#         with zipfile.ZipFile("../bytes/" + family + ".zip", "r") as zp:
#             list_files = zp.namelist()
#             for file in list_files:
#                 dataset["Id"].append(file)
#                 dataset["Class"].append(i)
#                 zp.extract(file, "./data/Andrea_Microsoft/")
#     df = pd.DataFrame(dataset)
#     df.to_csv("./data/Andrea_Microsoft_bytes.csv", index=False)


# def show_class_distribution(lbls: pd.DataFrame, title: str, dataset: str) -> None:
#     if dataset == "andrea_microsoft":
#         labels = ["IceId", "Magniber", "Ramnit", "Stop", "Trickbot", "Virlock", "Wannacry", "Zbot", "Benign"]
#     elif dataset == "benign":
#         labels = ["Ramnit", "Lollipop", "Kelihos_ver3", "Vundo", "Simda", "Tracur", "Kelihos_ver1", "Obfuscator.ACY",
#                   "Gatak", "Benign"]
#     elif dataset == "malign":
#         labels = ["Ramnit", "Lollipop", "Kelihos_ver3", "Vundo", "Simda", "Tracur", "Kelihos_ver1", "Obfuscator.ACY",
#                   "Gatak"]
#     nums = lbls["Class"].value_counts().sort_index()
#     pal = sns.color_palette("Blues", len(nums.values))
#     rank = nums.values.argsort().argsort()
#     palette = np.array(pal)[rank]
#     ax = sns.barplot(x=labels, y=nums.values, palette=palette)
#     ax.set(xlabel="Class", ylabel="Number of Samples", title=title)
#     plt.subplots_adjust(bottom=0.3)
#     plt.xticks(rotation=45)
#     plt.show()


def cross_validation(model, _X, _y, _cv=5) -> Any:
    scoring = "f1_weighted"
    results = cross_validate(estimator=model, X=_X, y=_y, cv=_cv, scoring=scoring, return_train_score=True)
    return results["test_score"]


# def plot_result(x_label, y_label, plot_title, val_data):
#     plt.figure(figsize=(12, 6))
#     labels = ["1st Fold", "2nd Fold", "3rd Fold", "4th Fold", "5th Fold"]
#     X_axis = np.arange(len(labels))
#     ax = plt.gca()
#     plt.ylim(0, 1)
#     plt.bar(X_axis + 0.2, val_data, 0.4, color='red', label='Validation')
#     plt.title(plot_title, fontsize=30)
#     plt.xticks(X_axis, labels)
#     plt.xlabel(x_label, fontsize=14)
#     plt.ylabel(y_label, fontsize=14)
#     plt.legend()
#     plt.grid(True)
#     plt.show()


def show_confusion_matrix(pred: np.ndarray, true: np.ndarray, title: str, dataset: str, feature: str) -> None:
    # if dataset == "andrea_microsoft":
    #     pred = pd.Categorical(pred, categories=range(0, 9))
    # else:
    #     pred = pd.Categorical(pred, categories=range(1, 10) if dataset == "malign" else range(0, 2))
    # matrix = pd.crosstab(true, pred, rownames=["Actual"], colnames=["Predict"], dropna=False)

    conf_matrix = confusion_matrix(true, pred)

    plt.figure(figsize=(12, 8))
    sns.set(font_scale=2)
    ax = sns.heatmap(conf_matrix, annot=True, fmt="d", cbar=False, cmap="Blues", vmax=175)
    ax.set(title=title)
    ax.set_xlabel("Predicted", labelpad=10)
    ax.set_ylabel("Actual", labelpad=20)
    plt.savefig("results/"+title+"_"+dataset+"_"+feature+".svg")


def fit_and_pred(clf, X_train, X_test, y_train, y_test, results, dataset, features):
    clf_result = cross_validation(clf, X_train, y_train)
    model_name = type(clf).__name__
    # plot_result(model_name, "F1", "F1 scores in 5 Folds", clf_result)

    clf.fit(X_train, y_train)
    clf_y_pred = clf.predict(X_test)
    if model_name == "RandomForestClassifier":
        title = "Random Forest"
    elif model_name == "KNeighborsClassifier":
        title = "K-NN"
    elif model_name == "SVC":
        title = "SVM"
    else:
        title = "Gradient Boosting"

    show_confusion_matrix(clf_y_pred, y_test["Class"], "Confusion Matrix of " + title, dataset=dataset, feature=features)
    accuracy = accuracy_score(y_test, clf_y_pred)
    print(f"Accuracy = {accuracy}")

    precision = precision_score(y_test, clf_y_pred)
    recall = recall_score(y_test, clf_y_pred)
    f1score = f1_score(y_test, clf_y_pred)  # , average='weighted')
    # stats.update(args.features, model_name, score)

    print(f"Precision = {precision}")
    print(f"Recall = {recall}")
    print(f"F1 Score = {f1score}")

    clf_y_pred = clf.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, clf_y_pred)
    auc = round(metrics.roc_auc_score(y_test, clf_y_pred), 4)

    print()

    print("---------------------------")

    results[model_name].extend([fpr, tpr, auc])

    return results


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ML for malware characterization and identification")
    parser.add_argument("--dataset",
                        type=str,
                        help="benign for dataset with benign file or malign for dataset without benign file",
                        choices=["benign", "malign", "andrea_microsoft"],
                        required=True)
    parser.add_argument("--features",
                        nargs="+",
                        help="insert feature(s) to use",
                        choices=["binary_size",
                                 "3_gram_bytes",
                                 "4_gram_opcode",
                                 "4_gram_API",
                                 "entropy",
                                 "opcode_counter",
                                 "API_check"],
                        required=True)
    args = parser.parse_args()
    ben_set = {"binary_size", "3_gram_bytes", "entropy"}

    mal_set = {"4_gram_opcode",
               "4_gram_API",
               "opcode_counter",
               "API_check"}

    not_andrea_microsoft_set = {"4_gram_opcode",
                                "4_gram_API",
                                "opcode_counter"}
    andrea_microsoft_set = {"API_check",
                            "3_gram_bytes",
                            "entropy"}

    # label_benign_file()

    feat_set = set(args.features)
    if args.dataset == "benign" and (feat_set & mal_set):
        print("Features with benign dataset: ")
        for i in ben_set:
            print(" - " + i)
        sys.exit()
    if args.dataset == "andrea_microsoft" and (feat_set & not_andrea_microsoft_set):
        print("Features with andrea microsoft dataset: ")
        for i in andrea_microsoft_set:
            print(" - " + i)
        sys.exit()

    # extract_files()

    if args.dataset == "benign":
        lbls = pd.read_csv(data_dir.joinpath("trainLabels_benign.csv"))
    elif args.dataset == "malign":
        lbls = pd.read_csv(data_dir.joinpath("trainLabels_malign.csv"))
    elif args.dataset == "andrea_microsoft":
        if "entropy" in args.features:
            lbls = pd.read_csv(data_dir.joinpath("Andrea_Microsoft_Entropy.csv"))
        elif "API_check" in args.features:
            lbls = pd.read_csv(data_dir.joinpath("Andrea_Microsoft_API.csv"))
        else:
            lbls = pd.read_csv(data_dir.joinpath("Andrea_Microsoft_bytes.csv"))
    else:
        sys.exit()

    lbls["Id"] = lbls["Id"].astype(str)
    lbls["Class"] = lbls["Class"].astype("category")
    lbls.set_index("Id", inplace=True)

    # show_class_distribution(lbls, "Distribution of classes", args.dataset)

    if args.dataset == "andrea_microsoft":
        if "API_check" in args.features and not os.path.exists("pre-extract/API_check_microsoft.npz"):
            start_time = datetime.now()
            if not os.path.exists("Utils/apis.txt"):
                for file in lbls.index:
                    extract_apis(file)
                save_apis()

            apis_vct = []
            load_apis()
            for file in lbls.index:
                apis_vct.append(check_apis(file))
            feature_to_save = np.array(apis_vct)
            col_sum = feature_to_save.sum(axis=0)
            top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
            print(col_sum[top_frqn_ftr_idx])
            save_npz(ftr_dir.joinpath("API_check_microsoft.npz"), csr_matrix(feature_to_save)[:, top_frqn_ftr_idx])
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)
        if "entropy" in args.features and not os.path.exists("pre-extract/entropy_microsoft.npz"):
            start_time = datetime.now()

            entropy_matrix = []
            for file in lbls.index:
                entropy_row = extract_entropy_section(file)
                entropy_matrix.append(entropy_row)

            feature_to_save = np.array(entropy_matrix)
            save_npz(ftr_dir.joinpath("entropy_microsoft.npz"), csr_matrix(feature_to_save))
            total_time = datetime.now() - start_time
            print("Time extraction entropy for Andrea dataset: %s" % total_time)
        if "3_gram_bytes" in args.features and not os.path.exists("pre-extract/3_gram_bytes_microsoft.npz"):
            start_time = datetime.now()
            bytes_ngram_vct, bytes_ngrams = extract_ngrams(lbls.index, extract_bytes_microsoft, 3)
            print(bytes_ngrams.shape)

            col_sum = bytes_ngrams.sum(axis=0).A1
            top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
            print(col_sum[top_frqn_ftr_idx])
            save_npz(ftr_dir.joinpath("3_gram_bytes_microsoft.npz"), bytes_ngrams[:, top_frqn_ftr_idx])
            total_time = datetime.now() - start_time
            print("Time extraction ngrams from Andrea dataset: %s" % total_time)
    else:
        if "binary_size" in args.features and not os.path.exists("pre-extract/binary_size_" + args.dataset + ".npz"):
            start_time = datetime.now()
            size_matrix = []
            for file in lbls.index:
                size_matrix.append([extract_binary_sizes(file)])
            feature_to_save = np.array(size_matrix)
            save_npz(ftr_dir.joinpath("binary_size_" + args.dataset + ".npz"), csr_matrix(feature_to_save))
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

        if "3_gram_bytes" in args.features and not os.path.exists("pre-extract/3_gram_bytes_" + args.dataset + ".npz"):
            start_time = datetime.now()
            bytes_ngram_vct, bytes_ngrams = extract_ngrams(lbls.index, extract_bytes_sequence, 3)
            print(bytes_ngrams.shape)

            col_sum = bytes_ngrams.sum(axis=0).A1
            top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
            print(col_sum[top_frqn_ftr_idx])
            save_npz(ftr_dir.joinpath("3_gram_bytes_" + args.dataset + ".npz"), bytes_ngrams[:, top_frqn_ftr_idx])
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

        if "4_gram_opcode" in args.features and not os.path.exists(
                "pre-extract/4_gram_opcode_malign.npz") and args.dataset == "malign":
            start_time = datetime.now()
            opcodes_ngram_vct, opcodes_ngrams = extract_ngrams(lbls.index, extract_opcode_sequence, 4)
            print(opcodes_ngrams.shape)

            col_sum = opcodes_ngrams.sum(axis=0).A1
            top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
            print(col_sum[top_frqn_ftr_idx])

            save_npz(ftr_dir.joinpath("4_gram_opcode_malign.npz"), opcodes_ngrams[:, top_frqn_ftr_idx])
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

        if "opcode_counter" in args.features and not os.path.exists(
                "pre-extract/opcode_counter_malign.npz") and args.dataset == "malign":
            start_time = datetime.now()
            opcode_counter = []

            for file in lbls.index:
                opc_counter = count_asm_opcode(file)
                opcode_counter.append(opc_counter)

            feature_to_save = np.array(opcode_counter)
            save_npz(ftr_dir.joinpath("opcode_counter_malign.npz"), csr_matrix(feature_to_save))
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

        if "API_check" in args.features and not os.path.exists(
                "pre-extract/API_check_malign.npz") and args.dataset == "malign":
            start_time = datetime.now()
            if not os.path.exists("Utils/syscalls.txt"):
                for file in lbls.index:
                    extract_syscall(file)
                save_syscall()

            syscall_vct = []
            load_syscall()
            for file in lbls.index:
                syscall_vct.append(check_syscall(file))

            feature_to_save = np.array(syscall_vct)

            col_sum = feature_to_save.sum(axis=0)
            top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
            print(col_sum[top_frqn_ftr_idx])

            save_npz(ftr_dir.joinpath("API_check_malign.npz"), csr_matrix(feature_to_save)[:, top_frqn_ftr_idx])
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)
        if "4_gram_API" in args.features and not os.path.exists(
                "pre-extract/4_gram_API_malign.npz") and args.dataset == "malign":
            start_time = datetime.now()
            api_ngrm_vct, api_ngrms = extract_ngrams(lbls.index, extract_syscall_sequence, 4)
            print(api_ngrms.shape)

            col_sum = api_ngrms.sum(axis=0).A1
            top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
            print(col_sum[top_frqn_ftr_idx])

            save_npz(ftr_dir.joinpath("4_gram_API_malign.npz"), api_ngrms[:, top_frqn_ftr_idx])
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

        if "entropy" in args.features and not os.path.exists("pre-extract/entropy_" + args.dataset + ".npz"):
            start_time = datetime.now()
            entropy_matrix = []
            for file in lbls.index:
                entropy_matrix.append([extract_entropy(file)])
            feature_to_save = np.array(entropy_matrix)
            save_npz(ftr_dir.joinpath("entropy_" + args.dataset + ".npz"), csr_matrix(feature_to_save))
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

    X = integrated_features(args.features, args.dataset)
    print(X.head())
    print(X.shape)

    print()

    stats = IndividualScoreStats("individual_scores.csv")
    stats.new_feature(args.features, len(X.columns))

    X_train, X_test, y_train, y_test = train_test_split(X, lbls, test_size=0.2, stratify=lbls)

    y_train = y_train.values.ravel()

    results = {"RandomForestClassifier": [],
               "KNeighborsClassifier": [],
               "SVC": [],
               "HistGradientBoostingClassifier": []}

    if len(args.features) == 1:
        feat = args.features[0]
    else:
        feat = "_".join(args.features)

    rf = RandomForestClassifier()
    print("Random Forest Results")
    results = fit_and_pred(rf, X_train, X_test, y_train, y_test, results, args.dataset, feat)

    knn = KNeighborsClassifier(n_neighbors=5)
    print("K-Nearest Neighbors Results")
    results = fit_and_pred(knn, X_train, X_test, y_train, y_test, results, args.dataset, feat)

    svm = svm.SVC(probability=True)
    print("Support Vector Machine Results")
    results = fit_and_pred(svm, X_train, X_test, y_train, y_test, results, args.dataset, feat)

    gb = HistGradientBoostingClassifier()
    print("Gradient Boosting Results")
    results = fit_and_pred(gb, X_train, X_test, y_train, y_test, results, args.dataset, feat)

    plt.figure(0).clf()
    plt.rcdefaults()
    plt.plot(results["RandomForestClassifier"][0],
             results["RandomForestClassifier"][1],
             label="Random Forest, AUC="+str(results["RandomForestClassifier"][2]))
    plt.plot(results["KNeighborsClassifier"][0],
             results["KNeighborsClassifier"][1],
             label="KNN, AUC=" + str(results["KNeighborsClassifier"][2]))
    plt.plot(results["SVC"][0],
             results["SVC"][1],
             label="SVM, AUC=" + str(results["SVC"][2]))
    plt.plot(results["HistGradientBoostingClassifier"][0],
             results["HistGradientBoostingClassifier"][1],
             label="Gradient Boosting, AUC=" + str(results["HistGradientBoostingClassifier"][2]))
    plt.legend(fontsize=12)
    plt.grid()
    plt.xlabel("False Positive Rate (Positive label: 1)", fontsize=12)
    plt.ylabel("True Positive Rate (Positive label: 1)", fontsize=12)
    plt.savefig("results/ROC_Curve_"+args.dataset+"_"+feat+".svg")

    # stats.save()
    # stats.plot(args.features)





