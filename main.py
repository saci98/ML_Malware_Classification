import os.path
import sys
from datetime import datetime
import zipfile
from typing import Any

import numpy as np
from scipy.sparse import save_npz, csr_matrix
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics, svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import ComplementNB
from matplotlib import pyplot as plt
import seaborn as sns
from Utils.Extract_feature import extract_ngrams, extract_bytes_sequence, extract_opcode_sequence, count_asm_opcode, \
    extract_syscall, save_syscall, check_syscall, load_syscall, extract_syscall_sequence, extract_entropy, extract_apis, \
    save_apis, load_apis, check_apis, extract_binary_sizes
from pathlib import Path
import pandas as pd
import argparse

from Utils.Integrate_features import integrated_features

data_dir = Path("data")
smp_dir = data_dir.joinpath("samples")
ftr_dir = Path("pre-extract")
if not ftr_dir.exists():
    ftr_dir.mkdir()


def extract_files():
    dataset = {
        "Id": [],
        "Class": []
    }
    families = ["IcedId", "Magniber", "Ramnit", "Stop", "Trickbot", "Virlock", "Wannacry", "Zbot", "Benign"]
    zips = os.listdir("../APIs/")
    for i, family in enumerate(families):
        with zipfile.ZipFile("../APIs/" + family + ".zip", "r") as zp:
            list_files = zp.namelist()
            for file in list_files:
                dataset["Id"].append(file)
                dataset["Class"].append(i)
                zp.extract(file, "./data/Andrea_Microsoft_dataset/")
    df = pd.DataFrame(dataset)
    df.to_csv("./data/Andrea_Microsoft_dataset.csv", index=False)


def show_class_distribution(lbls: pd.DataFrame, title: str, dataset: str) -> None:
    if dataset == "andrea_microsoft":
        labels = ["IceId", "Magniber", "Ramnit", "Stop", "Trickbot", "Virlock", "Wannacry", "Zbot", "Benign"]
    elif dataset == "benign":
        labels = ["Ramnit", "Lollipop", "Kelihos_ver3", "Vundo", "Simda", "Tracur", "Kelihos_ver1", "Obfuscator.ACY",
                  "Gatak", "Benign"]
    elif dataset == "malign":
        labels = ["Ramnit", "Lollipop", "Kelihos_ver3", "Vundo", "Simda", "Tracur", "Kelihos_ver1", "Obfuscator.ACY",
                  "Gatak"]
    nums = lbls["Class"].value_counts().sort_index()
    pal = sns.color_palette("Blues", len(nums.values))
    rank = nums.values.argsort().argsort()
    palette = np.array(pal)[rank]
    ax = sns.barplot(x=labels, y=nums.values, palette=palette)
    ax.set(xlabel="Class", ylabel="Number of Samples", title=title)
    plt.subplots_adjust(bottom=0.3)
    plt.xticks(rotation=45)
    plt.show()


def cross_validation(model, _X, _y, _cv=5) -> Any:
    scoring = "f1_weighted"
    results = cross_validate(estimator=model, X=_X, y=_y, cv=_cv, scoring=scoring, return_train_score=True)
    return results["test_score"]


def plot_result(x_label, y_label, plot_title, val_data):
    plt.figure(figsize=(12, 6))
    labels = ["1st Fold", "2nd Fold", "3rd Fold", "4th Fold", "5th Fold"]
    X_axis = np.arange(len(labels))
    ax = plt.gca()
    plt.ylim(0, 1)
    plt.bar(X_axis + 0.2, val_data, 0.4, color='red', label='Validation')
    plt.title(plot_title, fontsize=30)
    plt.xticks(X_axis, labels)
    plt.xlabel(x_label, fontsize=14)
    plt.ylabel(y_label, fontsize=14)
    plt.legend()
    plt.grid(True)
    plt.show()


def show_confusion_matrix(pred: np.ndarray, true: np.ndarray, title: str, dataset: str) -> None:
    if dataset == "andrea_microsoft":
        pred = pd.Categorical(pred, categories=range(0, 9))
    else:
        pred = pd.Categorical(pred, categories=range(1, 10 if dataset == "malign" else 11))
    matrix = pd.crosstab(true, pred, rownames=["Actual"], colnames=["Predict"], dropna=False)

    plt.figure(figsize=(12, 6))
    ax = sns.heatmap(matrix, annot=True, fmt="g", cmap="Blues")
    ax.set(title=title)
    plt.show()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ML for malware characterization and identification")
    parser.add_argument("--dataset",
                        type=str,
                        help="benign for dataset with benign file or malign for dataset without benign file",
                        choices=["benign", "malign", "andrea_microsoft"],
                        required=True)
    parser.add_argument("--features",
                        nargs="+",
                        help="insert feature(s) to use",
                        choices=["binary_size",
                                 "3_gram_bytes",
                                 "4_gram_opcode",
                                 "4_gram_API",
                                 "entropy",
                                 "opcode_counter",
                                 "API_check"],
                        required=True)
    args = parser.parse_args()
    ben_set = {"binary_size", "3_gram_bytes", "entropy"}

    mal_set = {"4_gram_opcode",
               "4_gram_API",
               "opcode_counter",
               "API_check"}

    not_andrea_microsoft_set = {"4_gram_opcode",
                                "4_gram_API",
                                "opcode_counter"}
    andrea_microsoft_set = {"API_check",
                            "3_gram_bytes",
                            "entropy"}

    # label_benign_file()
    feat_set = set(args.features)
    if args.dataset == "benign" and (feat_set & mal_set):
        print("Features with benign dataset: ")
        for i in ben_set:
            print(" - " + i)
        sys.exit()
    if args.dataset == "andrea_microsoft" and (feat_set & not_andrea_microsoft_set):
        print("Features with andrea microsoft dataset: ")
        for i in andrea_microsoft_set:
            print(" - " + i)
        sys.exit()

    # extract_files()

    if args.dataset == "benign":
        lbls = pd.read_csv(data_dir.joinpath("trainLabels_benign.csv"))
    elif args.dataset == "malign":
        lbls = pd.read_csv(data_dir.joinpath("trainLabels_malign.csv"))
    elif args.dataset == "andrea_microsoft":
        lbls = pd.read_csv(data_dir.joinpath("Andrea_Microsoft_dataset.csv"))
    else:
        sys.exit()

    lbls["Id"] = lbls["Id"].astype(str)
    lbls["Class"] = lbls["Class"].astype("category")
    lbls.set_index("Id", inplace=True)

    show_class_distribution(lbls, "Distribution of classes", args.dataset)

    if args.dataset == "andrea_microsoft":
        if "API_check" in args.features and not os.path.exists("pre-extract/API_check_microsoft.npz"):
            start_time = datetime.now()
            for file in lbls.index:
                extract_apis(file)
            save_apis()

            apis_vct = []
            load_apis()
            for file in lbls.index:
                apis_vct.append(check_apis(file))
            feature_to_save = np.array(apis_vct)
            col_sum = feature_to_save.sum(axis=0).A1
            top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
            print(col_sum[top_frqn_ftr_idx])
            save_npz(ftr_dir.joinpath("API_check_microsoft.npz"), csr_matrix(feature_to_save)[:, top_frqn_ftr_idx])
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)
    else:
        if "binary_size" in args.features and not os.path.exists("pre-extract/binary_size_" + args.dataset + ".npz"):
            start_time = datetime.now()
            size_matrix = []
            for file in lbls.index:
                size_matrix.append([extract_binary_sizes(file)])
            feature_to_save = np.array(size_matrix)
            save_npz(ftr_dir.joinpath("binary_size_" + args.dataset + ".npz"), csr_matrix(feature_to_save))
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

        if "3_gram_bytes" in args.features and not os.path.exists("pre-extract/3_gram_bytes_" + args.dataset + ".npz"):
            start_time = datetime.now()
            bytes_ngram_vct, bytes_ngrams = extract_ngrams(lbls.index, extract_bytes_sequence, 3)
            print(bytes_ngrams.shape)

            col_sum = bytes_ngrams.sum(axis=0).A1
            top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
            print(col_sum[top_frqn_ftr_idx])
            save_npz(ftr_dir.joinpath("3_gram_bytes_" + args.dataset + ".npz"), bytes_ngrams[:, top_frqn_ftr_idx])
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

        if "4_gram_opcode" in args.features and not os.path.exists(
                "pre-extract/4_gram_opcode_malign.npz") and args.dataset == "malign":
            start_time = datetime.now()
            opcodes_ngram_vct, opcodes_ngrams = extract_ngrams(lbls.index, extract_opcode_sequence, 4)
            print(opcodes_ngrams.shape)

            col_sum = opcodes_ngrams.sum(axis=0).A1
            top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
            print(col_sum[top_frqn_ftr_idx])

            save_npz(ftr_dir.joinpath("4_gram_opcode_malign.npz"), opcodes_ngrams[:, top_frqn_ftr_idx])
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

        if "opcode_counter" in args.features and not os.path.exists(
                "pre-extract/opcode_counter_malign.npz") and args.dataset == "malign":
            start_time = datetime.now()
            opcode_counter = []

            for file in lbls.index:
                opc_counter = count_asm_opcode(file)
                opcode_counter.append(opc_counter)

            feature_to_save = np.array(opcode_counter)
            save_npz(ftr_dir.joinpath("opcode_counter_malign.npz"), csr_matrix(feature_to_save))
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

        if "API_check" in args.features and not os.path.exists(
                "pre-extract/API_check_malign.npz") and args.dataset == "malign":
            start_time = datetime.now()
            for file in lbls.index:
                extract_syscall(file)
            save_syscall()

            syscall_vct = []
            load_syscall()
            for file in lbls.index:
                syscall_vct.append(check_syscall(file))

            feature_to_save = np.array(syscall_vct)
            save_npz(ftr_dir.joinpath("API_check_malign.npz"), csr_matrix(feature_to_save))
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)
        if "4_gram_API" in args.features and not os.path.exists(
                "pre-extract/4_gram_API_malign.npz") and args.dataset == "malign":
            start_time = datetime.now()
            api_ngrm_vct, api_ngrms = extract_ngrams(lbls.index, extract_syscall_sequence, 4)
            print(api_ngrms.shape)

            col_sum = api_ngrms.sum(axis=0).A1
            top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
            print(col_sum[top_frqn_ftr_idx])

            save_npz(ftr_dir.joinpath("4_gram_API_malign.npz"), api_ngrms[:, top_frqn_ftr_idx])
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

        if "entropy" in args.features and not os.path.exists("pre-extract/entropy_" + args.dataset + ".npz"):
            start_time = datetime.now()
            entropy_matrix = []
            for file in lbls.index:
                entropy_matrix.append([extract_entropy(file)])
            feature_to_save = np.array(entropy_matrix)
            save_npz(ftr_dir.joinpath("entropy_" + args.dataset + ".npz"), csr_matrix(feature_to_save))
            total_time = datetime.now() - start_time
            print("Time extraction API check: %s" % total_time)

    X = integrated_features(args.features, args.dataset)
    print(X.head())

    X_train, X_test, y_train, y_test = train_test_split(X, lbls, test_size=0.2, stratify=lbls)

    y_train = y_train.values.ravel()

    rf = RandomForestClassifier()

    rf_result = cross_validation(rf, X_train, y_train, 5)
    model_name = "Random Forest"
    plot_result(model_name, "F1", "F1 scores in 5 Folds", rf_result)

    rf.fit(X_train, y_train)
    rf_y_pred = rf.predict(X_test)
    print("RF F1 score for each class: ", metrics.f1_score(y_test, rf_y_pred, average=None))
    print("RF F1 score (macro): ", metrics.f1_score(y_test, rf_y_pred, average='weighted'))
    print()

    show_confusion_matrix(rf_y_pred, y_test["Class"], "Confusion Matrix of RF", dataset=args.dataset)

    print("---------------------------")

    knn = KNeighborsClassifier(n_neighbors=5)

    knn_result = cross_validation(knn, X_train, y_train, 5)
    model_name = "K-Nearest Neighbour"
    plot_result(model_name, "F1", "F1 scores in 5 Folds", knn_result)

    knn.fit(X_train, y_train)
    knn_y_pred = knn.predict(X_test)
    print("KNN F1 score for each class: ", metrics.f1_score(y_test, knn_y_pred, average=None))
    print("KNN F1 score (macro): ", metrics.f1_score(y_test, knn_y_pred, average='weighted'))
    print()

    show_confusion_matrix(knn_y_pred, y_test["Class"], "Confusion Matrix of KNN", dataset=args.dataset)

    print("---------------------------")

    svm = svm.SVC()

    svm_result = cross_validation(svm, X_train, y_train, 5)
    model_name = "Support Vector Machine"
    plot_result(model_name, "F1", "F1 scores in 5 Folds", svm_result)

    svm.fit(X_train, y_train)
    svm_y_pred = svm.predict(X_test)
    print("SVM F1 score for each class: ", metrics.f1_score(y_test, svm_y_pred, average=None))
    print("SVM F1 score (macro): ", metrics.f1_score(y_test, svm_y_pred, average='weighted'))
    print()

    show_confusion_matrix(svm_y_pred, y_test["Class"], "Confusion Matrix of SVM", dataset=args.dataset)

    print("---------------------------")

    nb = ComplementNB()

    nb_result = cross_validation(nb, X_train, y_train, 5)
    model_name = "Naive Bayes"
    plot_result(model_name, "F1", "F1 scores in 5 Folds", nb_result)

    nb.fit(X_train, y_train)
    nb_y_pred = nb.predict(X_test)
    print("NB F1 score for each class: ", metrics.f1_score(y_test, nb_y_pred, average=None))
    print("NB F1 score (macro): ", metrics.f1_score(y_test, nb_y_pred, average='weighted'))
    print()

    show_confusion_matrix(nb_y_pred, y_test["Class"], "Confusion Matrix of NB", dataset=args.dataset)
