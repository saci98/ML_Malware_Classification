import os.path
import sys
from typing import Any

import numpy as np
import py7zr
from scipy.sparse import save_npz, csr_matrix
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics, svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import ComplementNB
from matplotlib import pyplot as plt
import seaborn as sns
from Utils.Extract_feature import extract_ngrams, extract_bytes_sequence, extract_opcode_sequence, count_asm_opcode, \
    extract_syscall, save_syscall, check_syscall, load_syscall, extract_syscall_sequence, extract_entropy
from pathlib import Path
import pandas as pd
import argparse

from Utils.Integrate_features import integrated_features

data_dir = Path("data")
smp_dir = data_dir.joinpath("samples")
ftr_dir = Path("pre-extract")
if not ftr_dir.exists():
    ftr_dir.mkdir()


def extract_files():
    APIs = py7zr.SevenZipFile("../APIs.7z")
    APIs.extractall("..")


def show_class_distribution(lbls: pd.DataFrame, title: str) -> None:
    labels = ["Ramnit", "Lollipop", "Kelihos_ver3", "Vundo", "Simda", "Tracur", "Kelihos_ver1", "Obfuscator.ACY", "Gatak", "Benign"]
    nums = lbls["Class"].value_counts().sort_index()
    pal = sns.color_palette("Blues", len(nums.values))
    rank = nums.values.argsort().argsort()
    palette = np.array(pal)[rank]
    ax = sns.barplot(x=labels, y=nums.values, palette=palette)
    ax.set(xlabel="Class", ylabel="Number of Samples", title=title)
    plt.subplots_adjust(bottom=0.3)
    plt.xticks(rotation=45)
    plt.show()

def cross_validation(model, _X, _y, _cv=5) -> Any:
    scoring = "f1_weighted"
    results = cross_validate(estimator=model, X=_X, y=_y, cv=_cv, scoring=scoring, return_train_score=True)
    return results["test_score"]


def plot_result(x_label, y_label, plot_title, val_data):
    plt.figure(figsize=(12,6))
    labels=["1st Fold", "2nd Fold", "3rd Fold", "4th Fold", "5th Fold"]
    X_axis = np.arange(len(labels))
    ax = plt.gca()
    plt.ylim(0, 1)
    plt.bar(X_axis + 0.2, val_data, 0.4, color='red', label='Validation')
    plt.title(plot_title, fontsize=30)
    plt.xticks(X_axis, labels)
    plt.xlabel(x_label, fontsize=14)
    plt.ylabel(y_label, fontsize=14)
    plt.legend()
    plt.grid(True)
    plt.show()


def show_confusion_matrix(pred: np.ndarray, true: np.ndarray, title: str, dataset: str) -> None:
    pred = pd.Categorical(pred, categories=range(1, 10 if dataset == "malign" else 11))
    matrix = pd.crosstab(true, pred, rownames=["Actual"], colnames=["Predict"], dropna=False)

    plt.figure(figsize=(12, 6))
    ax = sns.heatmap(matrix, annot=True, fmt="g", cmap="Blues")
    ax.set(title=title)
    plt.show()


if __name__ == '__main__':
    # parser = argparse.ArgumentParser(description="ML for malware characterization and identification")
    # parser.add_argument("--dataset",
    #                     type=str,
    #                     help="benign for dataset with benign file or malign for dataset without benign file",
    #                     choices=["benign", "malign"],
    #                     required=True)
    # parser.add_argument("--features",
    #                     nargs="+",
    #                     help="insert feature(s) to use",
    #                     choices=["3_gram_bytes",
    #                              "4_gram_opcode",
    #                              "4_gram_API",
    #                              "entropy",
    #                              "opcode_counter",
    #                              "API_check"],
    #                     required=True)
    # args = parser.parse_args()
    # ben_set = {"3_gram_bytes", "entropy"}
    #
    # mal_set = {"4_gram_opcode",
    #            "4_gram_API",
    #            "opcode_counter",
    #            "API_check"}
    # # label_benign_file()
    # feat_set = set(args.features)
    # if args.dataset == "benign" and (feat_set & mal_set):
    #     print("Features with benign dataset: ")
    #     for i in ben_set:
    #         print(" - " + i)
    #     sys.exit()

    extract_files()

    # if args.dataset == "benign":
    #     lbls = pd.read_csv(data_dir.joinpath("trainLabels_benign.csv"))
    # elif args.dataset == "malign":
    #     lbls = pd.read_csv(data_dir.joinpath("trainLabels_malign.csv"))
    # else:
    #     sys.exit()
    #
    # lbls["Id"] = lbls["Id"].astype(str)
    # lbls["Class"] = lbls["Class"].astype("category")
    # lbls.set_index("Id", inplace=True)
    #
    # show_class_distribution(lbls, "Distribution of classes")
    #
    # if "3_gram_bytes" in args.features and not os.path.exists("pre-extract/3_gram_bytes_"+args.dataset+".npz"):
    #     bytes_ngram_vct, bytes_ngrams = extract_ngrams(lbls.index, extract_bytes_sequence, 3)
    #     print(bytes_ngrams.shape)
    #
    #     col_sum = bytes_ngrams.sum(axis=0).A1
    #     top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
    #     print(col_sum[top_frqn_ftr_idx])
    #     save_npz(ftr_dir.joinpath("3_gram_bytes_"+args.dataset+".npz"), bytes_ngrams[:, top_frqn_ftr_idx])
    #
    # if "4_gram_opcode" in args.features and not os.path.exists("pre-extract/4_gram_opcode_malign.npz") and args.dataset == "malign":
    #     opcodes_ngram_vct, opcodes_ngrams = extract_ngrams(lbls.index, extract_opcode_sequence, 4)
    #     print(opcodes_ngrams.shape)
    #
    #     col_sum = opcodes_ngrams.sum(axis=0).A1
    #     top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
    #     print(col_sum[top_frqn_ftr_idx])
    #
    #     save_npz(ftr_dir.joinpath("4_gram_opcode_malign.npz"), opcodes_ngrams[:, top_frqn_ftr_idx])
    #
    # if "opcode_counter" in args.features and not os.path.exists("pre-extract/opcode_counter_malign.npz") and args.dataset == "malign":
    #     opcode_counter = []
    #
    #     for file in lbls.index:
    #         opc_counter = count_asm_opcode(file)
    #         opcode_counter.append(opc_counter)
    #
    #     feature_to_save = np.array(opcode_counter)
    #     save_npz(ftr_dir.joinpath("opcode_counter_malign.npz"), csr_matrix(feature_to_save))
    #
    # if "API_check" in args.features and not os.path.exists("pre-extract/API_check_malign.npz") and args.dataset == "malign":
    #     for file in lbls.index:
    #         extract_syscall(file)
    #     save_syscall()
    #
    #     syscall_vct = []
    #     load_syscall()
    #     for file in lbls.index:
    #         syscall_vct.append(check_syscall(file))
    #
    #     feature_to_save = np.array(syscall_vct)
    #     save_npz(ftr_dir.joinpath("API_check_malign.npz"), csr_matrix(feature_to_save))
    #
    # if "4_gram_API" in args.features and not os.path.exists("pre-extract/4_gram_API_malign.npz") and args.dataset == "malign":
    #     api_ngrm_vct, api_ngrms = extract_ngrams(lbls.index, extract_syscall_sequence, 4)
    #     print(api_ngrms.shape)
    #
    #     col_sum = api_ngrms.sum(axis=0).A1
    #     top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
    #     print(col_sum[top_frqn_ftr_idx])
    #
    #     save_npz(ftr_dir.joinpath("4_gram_API_malign.npz"), api_ngrms[:, top_frqn_ftr_idx])
    #
    # if "entropy" in args.features and not os.path.exists("pre-extract/entropy_"+args.dataset+".npz"):
    #     entropy_matrix = []
    #     for file in lbls.index:
    #         entropy_matrix.append([extract_entropy(file)])
    #     feature_to_save = np.array(entropy_matrix)
    #     save_npz(ftr_dir.joinpath("entropy_"+args.dataset+".npz"), csr_matrix(feature_to_save))
    #
    # X = integrated_features(args.features, args.dataset)
    # print(X.head())
    #
    # X_train, X_test, y_train, y_test = train_test_split(X, lbls, test_size=0.2, stratify=lbls)
    #
    # y_train = y_train.values.ravel()
    #
    # rf = RandomForestClassifier()
    #
    # rf_result = cross_validation(rf, X_train, y_train, 5)
    # model_name = "Random Forest"
    # plot_result(model_name, "F1", "F1 scores in 5 Folds", rf_result)
    #
    # rf.fit(X_train, y_train)
    # rf_y_pred = rf.predict(X_test)
    # print("RF F1 score for each class: ", metrics.f1_score(y_test, rf_y_pred, average=None))
    # print("RF F1 score (macro): ", metrics.f1_score(y_test, rf_y_pred, average='weighted'))
    # print()
    #
    # show_confusion_matrix(rf_y_pred, y_test["Class"], "Confusion Matrix of RF", dataset=args.dataset)
    #
    # print("---------------------------")
    #
    # knn = KNeighborsClassifier(n_neighbors=5)
    #
    # knn_result = cross_validation(knn, X_train, y_train, 5)
    # model_name = "K-Nearest Neighbour"
    # plot_result(model_name, "F1", "F1 scores in 5 Folds", knn_result)
    #
    # knn.fit(X_train, y_train)
    # knn_y_pred = knn.predict(X_test)
    # print("KNN F1 score for each class: ", metrics.f1_score(y_test, knn_y_pred, average=None))
    # print("KNN F1 score (macro): ", metrics.f1_score(y_test, knn_y_pred, average='weighted'))
    # print()
    #
    # show_confusion_matrix(knn_y_pred, y_test["Class"], "Confusion Matrix of KNN", dataset=args.dataset)
    #
    # print("---------------------------")
    #
    # svm = svm.SVC()
    #
    # svm_result = cross_validation(svm, X_train, y_train, 5)
    # model_name = "Support Vector Machine"
    # plot_result(model_name, "F1", "F1 scores in 5 Folds", svm_result)
    #
    # svm.fit(X_train, y_train)
    # svm_y_pred = svm.predict(X_test)
    # print("SVM F1 score for each class: ", metrics.f1_score(y_test, svm_y_pred, average=None))
    # print("SVM F1 score (macro): ", metrics.f1_score(y_test, svm_y_pred, average='weighted'))
    # print()
    #
    # show_confusion_matrix(svm_y_pred, y_test["Class"], "Confusion Matrix of SVM", dataset=args.dataset)
    #
    # print("---------------------------")
    #
    # nb = ComplementNB()
    #
    # nb_result = cross_validation(nb, X_train, y_train, 5)
    # model_name = "Naive Bayes"
    # plot_result(model_name, "F1", "F1 scores in 5 Folds", nb_result)
    #
    # nb.fit(X_train, y_train)
    # nb_y_pred = nb.predict(X_test)
    # print("NB F1 score for each class: ", metrics.f1_score(y_test, nb_y_pred, average=None))
    # print("NB F1 score (macro): ", metrics.f1_score(y_test, nb_y_pred, average='weighted'))
    # print()
    #
    # show_confusion_matrix(nb_y_pred, y_test["Class"], "Confusion Matrix of NB", dataset=args.dataset)
