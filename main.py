import os.path
import sys
from typing import List

import numpy as np
from scipy.sparse import save_npz, csr_matrix
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics, svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import ComplementNB

from Utils.Extract_feature import extract_ngrams, extract_bytes_sequence, extract_opcode_sequence, count_asm_opcode, \
    extract_syscall, save_syscall, check_syscall, load_syscall, extract_syscall_sequence, extract_entropy
from pathlib import Path
import pandas as pd
import argparse

from Utils.Integrate_features import integrated_features

data_dir = Path("data")
smp_dir = data_dir.joinpath("samples")
ftr_dir = Path("pre-extract")
if not ftr_dir.exists():
    ftr_dir.mkdir()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ML for malware characterization and identification")
    parser.add_argument("--dataset",
                        type=str,
                        help="benign for dataset with benign file or malign for dataset without benign file",
                        choices=["benign", "malign"],
                        required=True)
    parser.add_argument("--features",
                        nargs="+",
                        help="insert feature(s) to use",
                        choices=["3_gram_bytes",
                                 "4_gram_opcode",
                                 "4_gram_API",
                                 "entropy",
                                 "opcode_counter",
                                 "API_check"],
                        required=True)
    args = parser.parse_args()
    ben_set = {"4_gram_bytes", "entropy"}

    mal_set = {"4_gram_opcode",
               "4_gram_API",
               "opcode_counter",
               "API_check"}
    # label_benign_file()
    feat_set = set(args.features)
    if args.dataset == "benign" and (feat_set & mal_set):
        print("Features with benign dataset: ")
        for i in ben_set:
            print(" - " + i)
        sys.exit()

    if args.dataset == "benign":
        lbls = pd.read_csv(data_dir.joinpath("trainLabels_benign.csv"))
    elif args.dataset == "malign":
        lbls = pd.read_csv(data_dir.joinpath("trainLabels_malign.csv"))
    else:
        sys.exit()

    lbls["Id"] = lbls["Id"].astype(str)
    lbls["Class"] = lbls["Class"].astype("category")
    lbls.set_index("Id", inplace=True)

    if "3_gram_bytes" in args.features and not os.path.exists("pre-extract/3_gram_bytes_"+args.dataset+".npz"):
        bytes_ngram_vct, bytes_ngrams = extract_ngrams(lbls.index, extract_bytes_sequence, 3)
        print(bytes_ngrams.shape)

        col_sum = bytes_ngrams.sum(axis=0).A1
        top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
        print(col_sum[top_frqn_ftr_idx])
        save_npz(ftr_dir.joinpath("3_gram_bytes_"+args.dataset+".npz"), bytes_ngrams[:, top_frqn_ftr_idx])

    if "4_gram_opcode" in args.features and not os.path.exists("pre-extract/4_gram_opcode_malign.npz") and args.dataset == "malign":
        opcodes_ngram_vct, opcodes_ngrams = extract_ngrams(lbls.index, extract_opcode_sequence, 4)
        print(opcodes_ngrams.shape)

        col_sum = opcodes_ngrams.sum(axis=0).A1
        top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
        print(col_sum[top_frqn_ftr_idx])

        save_npz(ftr_dir.joinpath("4_gram_opcode_malign.npz"), opcodes_ngrams[:, top_frqn_ftr_idx])

    if "opcode_counter" in args.features and not os.path.exists("pre-extract/opcode_counter_malign.npz") and args.dataset == "malign":
        opcode_counter = []

        for file in lbls.index:
            opc_counter = count_asm_opcode(file)
            opcode_counter.append(opc_counter)

        feature_to_save = np.array(opcode_counter)
        save_npz(ftr_dir.joinpath("opcode_counter_malign.npz"), csr_matrix(feature_to_save))

    if "API_check" in args.features and not os.path.exists("pre-extract/API_check_malign.npz") and args.dataset == "malign":
        for file in lbls.index:
            extract_syscall(file)
        save_syscall()

        syscall_vct = []
        load_syscall()
        for file in lbls.index:
            syscall_vct.append(check_syscall(file))

        feature_to_save = np.array(syscall_vct)
        save_npz(ftr_dir.joinpath("API_check_malign.npz"), csr_matrix(feature_to_save))

    if "4_gram_API" in args.features and not os.path.exists("pre-extract/4_gram_API_malign.npz") and args.dataset == "malign":
        api_ngrm_vct, api_ngrms = extract_ngrams(lbls.index, extract_syscall_sequence, 4)
        print(api_ngrms.shape)

        col_sum = api_ngrms.sum(axis=0).A1
        top_frqn_ftr_idx = np.argsort(col_sum)[::-1][:5000]
        print(col_sum[top_frqn_ftr_idx])

        save_npz(ftr_dir.joinpath("4_gram_API_malign.npz"), api_ngrms[:, top_frqn_ftr_idx])

    if "entropy" in args.features and not os.path.exists("pre-extract/entropy_"+args.dataset+".npz"):
        entropy_matrix = []
        for file in lbls.index:
            entropy_matrix.append([extract_entropy(file)])
        feature_to_save = np.array(entropy_matrix)
        save_npz(ftr_dir.joinpath("entropy_"+args.dataset+".npz"), csr_matrix(feature_to_save))

    X = integrated_features(args.features, args.dataset)
    print(X.head())

    X_train, X_test, y_train, y_test = train_test_split(X, lbls, test_size=0.2, stratify=lbls)

    y_train = y_train.values.ravel()

    clf = RandomForestClassifier()
    clf.fit(X_train, y_train)
    clf_y_pred = clf.predict(X_test)
    print("RF F1 score for each class: ", metrics.f1_score(y_test, clf_y_pred, average=None))
    print("RF F1 score (macro): ", metrics.f1_score(y_test, clf_y_pred, average='weighted'))
    print()
    print("RF Confusion Metrix:\n", metrics.confusion_matrix(y_test, clf_y_pred))

    print("---------------------------")

    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train, y_train)
    knn_y_pred = knn.predict(X_test)
    print("KNN F1 score for each class: ", metrics.f1_score(y_test, knn_y_pred, average=None))
    print("KNN F1 score (macro): ", metrics.f1_score(y_test, knn_y_pred, average='weighted'))
    print()
    print("K-NN Confusion Metrix:\n", metrics.confusion_matrix(y_test, knn_y_pred))

    print("---------------------------")

    svm = svm.SVC()
    svm.fit(X_train, y_train)
    svm_y_pred = svm.predict(X_test)
    print("SVM F1 score for each class: ", metrics.f1_score(y_test, svm_y_pred, average=None))
    print("SVM F1 score (macro): ", metrics.f1_score(y_test, svm_y_pred, average='weighted'))
    print()
    print("SVM Confusion Metrix:\n", metrics.confusion_matrix(y_test, svm_y_pred))

    print("---------------------------")

    nb = ComplementNB()
    nb.fit(X_train, y_train)
    nb_y_pred = nb.predict(X_test)
    print("NB F1 score for each class: ", metrics.f1_score(y_test, nb_y_pred, average=None))
    print("NB F1 score (macro): ", metrics.f1_score(y_test, nb_y_pred, average='weighted'))
    print()
    print("NB Confusion Metrix:\n", metrics.confusion_matrix(y_test, nb_y_pred))


